{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EJY5bI1VC2gg"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=pickle.load(open('sentiment_lstm.pkl','rb'))"
      ],
      "metadata": {
        "id": "XVW9IW8LDbnG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tfidf.pkl', 'rb') as tfidf_file:\n",
        "    tfidf = pickle.load(tfidf_file)  # TF-IDF vectorizer\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Function to preprocess and vectorize input\n",
        "def preprocess_and_vectorize(sentence):\n",
        "    preprocessed = preprocess_text(sentence)\n",
        "    vectorized = tfidf.transform([preprocessed])  # Transform into TF-IDF vector\n",
        "    return vectorized\n",
        "\n",
        "def preprocess_and_reshape_for_lstm(sentence):\n",
        "    vectorized_input = preprocess_and_vectorize(sentence)  # Output shape: (1, num_features)\n",
        "    reshaped_input = vectorized_input.toarray().reshape(1, 1, -1)  # Reshape to (batch_size, timesteps, features)\n",
        "    return reshaped_input\n",
        "\n",
        "# Test a new sentence\n",
        "def test_models(sentence):\n",
        "    # Preprocess and vectorize the sentence\n",
        "    vectorized_input = preprocess_and_vectorize(sentence)\n",
        "    lstm_input = preprocess_and_reshape_for_lstm(sentence)\n",
        "    lstm_prediction = model.predict(lstm_input)\n",
        "    lstm_label = np.argmax(lstm_prediction, axis=1)[0]\n",
        "    return lstm_label"
      ],
      "metadata": {
        "id": "xsqjlACkC_T0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1oBpzSneIOzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = \"The world is not good\"\n",
        "result = test_models(new_sentence)\n",
        "\n",
        "# Map numeric predictions to sentiments (if necessary)\n",
        "sentiment_mapping ={0: 'Irrelevant', 1: 'Negative', 2: 'Neutral', 3: 'Positive'}\n",
        "sentiment = sentiment_mapping.get(result,'unknown')\n",
        "\n",
        "# print(f\"Random Forest Prediction: {rf_result}\")\n",
        "print(sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGzvXeSuDuHK",
        "outputId": "ce9a57c4-b462-4d16-8833-364dae704864"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp2IgYHrEHeo",
        "outputId": "1cbb255e-de10-4d01-a0e9-72f2324b9f62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model=pickle.load(open('model2.pkl','rb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "_VgBWRmVa8QV",
        "outputId": "92336db0-b993-4061-c062-2ec5f273824f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '\\xff'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-61fd95769f43>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model2.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\xff'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tfidf_vec.pkl', 'rb') as tfidf_file:\n",
        "    tfidf = pickle.load(tfidf_file)  # TF-IDF vectorizer\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Function to preprocess and vectorize input\n",
        "def preprocess_and_vectorize(sentence):\n",
        "    preprocessed = preprocess_text(sentence)\n",
        "    vectorized = tfidf.transform([preprocessed])  # Transform into TF-IDF vector\n",
        "    return vectorized\n",
        "\n",
        "# Test a new sentence\n",
        "def test_models(sentence):\n",
        "    # Preprocess and vectorize the sentence\n",
        "    vectorized_input = preprocess_and_vectorize(sentence)\n",
        "\n",
        "    # Random Forest prediction\n",
        "    rf_prediction = rf_model.predict(vectorized_input)\n",
        "    rf_label = rf_prediction[0] # Assuming label is directly predicted\n",
        "    return rf_label"
      ],
      "metadata": {
        "id": "808YkS6aEN1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}